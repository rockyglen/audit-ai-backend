import os
import json
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


# --- CUSTOM WRAPPER TO FIX GROQ n=1 ISSUE ---
class GroqSingleton(ChatGroq):
    """
    Forces n=1 because Groq API does not support n>1 yet.
    RAGAS tries to ask for n=3 for some metrics (like answer_relevancy).
    """

    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        if "n" in kwargs and kwargs["n"] > 1:
            kwargs["n"] = 1
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        if "n" in kwargs and kwargs["n"] > 1:
            kwargs["n"] = 1
        return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)


def run_ragas_eval():
    # 1. Load Data (Generated by collector.py)
    file_path = "rag_results.json"
    if not os.path.exists(file_path):
        print(f"‚ùå Error: '{file_path}' not found.")
        print("   Please run 'python collector.py' first to generate the dataset.")
        return

    with open(file_path, "r") as f:
        raw_data = json.load(f)

    print(f"üìÇ Loaded {len(raw_data)} records from {file_path}")

    # ‚ö†Ô∏è LIMIT DATASET TO SAVE TOKENS (Rate Limit Protection)
    # We slice to the first 3 records for testing. Remove [:3] to run all.
    raw_data = raw_data[:3]
    print(f"‚ö†Ô∏è  Running on first {len(raw_data)} records to respect Rate Limits...")

    # 2. Prepare Dataset for RAGAS
    # RAGAS expects a HuggingFace Dataset with columns:
    # ['question', 'answer', 'contexts', 'ground_truth']
    ragas_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": [],
    }

    for entry in raw_data:
        ragas_data["question"].append(entry["question"])
        ragas_data["answer"].append(entry["answer"])
        ragas_data["contexts"].append(entry["contexts"])
        # RAGAS expects ground_truth to be a string (or list of strings)
        ragas_data["ground_truth"].append(entry["ground_truth"])

    dataset = Dataset.from_dict(ragas_data)

    # 3. Configure Judge Models (Reuse your existing stack)
    # Judge LLM: Llama 3 via Groq
    judge_llm = GroqSingleton(
        model="llama-3.3-70b-versatile",
        temperature=0,
        groq_api_key=os.getenv("GROQ_API_KEY"),
    )

    # Embeddings: Gemini (Needed for Answer Relevancy)
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/gemini-embedding-001",
        google_api_key=os.getenv("GOOGLE_API_KEY"),
    )

    # 4. Run Evaluation
    print("üöÄ Starting RAGAS Evaluation...")
    results = evaluate(
        dataset=dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ],
        llm=judge_llm,
        embeddings=embeddings,
        raise_exceptions=False,
        # ‚ö†Ô∏è THROTTLE REQUESTS
        run_config=RunConfig(
            max_workers=1,  # Run sequentially to avoid hitting Rate Limits (RPM)
        ),
    )

    # 5. Save Results
    print("\n--- üìä RAGAS SCORES ---")
    print(results)

    df = results.to_pandas()
    df.to_csv("ragas_results.csv", index=False)
    print("\n‚úÖ Detailed results saved to 'ragas_results.csv'")


if __name__ == "__main__":
    run_ragas_eval()