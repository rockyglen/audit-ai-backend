import os
import json
import asyncio
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Import the graph AND the router logic
from rag_engine import app as audit_graph, route_query, run_chat_logic

app = FastAPI(
    title="AuditAI Agent API",
    description="A Streaming Agentic RAG API for NIST Compliance",
    version="2.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    query: str
    history: Optional[List[Dict[str, str]]] = []


async def run_agent_stream(query: str):
    """
    Robust Generator: Streams ANY text generated by the model.
    """

    # --- 1. ROUTER (Fast Path) ---
    intent = route_query(query)

    if intent == "chat":
        response = run_chat_logic(query)
        answer_text = response["answer"]
        tokens = answer_text.split(" ")
        for token in tokens:
            payload = json.dumps({"type": "token", "content": token + " "})
            yield f"{payload}\n"
            await asyncio.sleep(0.05)
        payload = json.dumps({"type": "sources", "content": []})
        yield f"{payload}\n"
        return

    # --- 2. GRAPH (Search Path) ---
    captured_sources = []

    try:
        # Stream events from the graph
        async for event in audit_graph.astream_events(
            {"question": query}, version="v1"
        ):
            kind = event["event"]
            data = event.get("data", {})

            # A. Capture Sources (When retrieval finishes)
            if kind == "on_chain_end" and event.get("name") == "retrieve":
                if "output" in data and data["output"]:
                    docs = data["output"].get("documents", [])
                    captured_sources = [
                        {
                            "file": d.metadata.get("source_file", "Unknown Doc"),
                            "page": d.metadata.get("page", 0),
                            "text": d.page_content[:200] + "...",
                        }
                        for d in docs
                    ]

            # B. Capture Tokens (The "Universal" Logic)
            # We look for ANY event that has a "chunk" with "content".
            # This ignores the event name (on_chat_model_stream vs on_llm_stream).
            if "chunk" in data:
                chunk = data["chunk"]

                # Extract content whether it's an Object or a Dictionary
                content = ""
                if hasattr(chunk, "content"):
                    content = chunk.content
                elif isinstance(chunk, dict) and "content" in chunk:
                    content = chunk["content"]

                if content:
                    # Send the token to the frontend
                    payload = json.dumps({"type": "token", "content": content})
                    yield f"{payload}\n"

    except Exception as e:
        print(f"Graph Error: {e}")
        # Send error as text so user sees it
        err_payload = json.dumps(
            {"type": "token", "content": f"\n[System Error: {str(e)}]"}
        )
        yield f"{err_payload}\n"

    # Send Sources at the end
    if captured_sources:
        payload = json.dumps({"type": "sources", "content": captured_sources})
        yield f"{payload}\n"


@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    return StreamingResponse(
        run_agent_stream(request.query), media_type="application/x-ndjson"
    )


@app.get("/health")
def health_check():
    return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn

    if not os.getenv("GROQ_API_KEY"):
        print("‚ùå Error: GROQ_API_KEY is missing!")
    else:
        print("üöÄ Starting AuditAI Agent...")
        uvicorn.run(app, host="0.0.0.0", port=8000)
