import sys
import os

# --- PATH HACK (Industrial Standard for standalone scripts) ---
# Adds the 'src' directory to the path so we can import 'audit_ai'
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
sys.path.append(os.path.join(PROJECT_ROOT, "src"))

import json
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.run_config import RunConfig
from ragas.metrics import (
    Faithfulness,
    AnswerRelevancy,
    ContextPrecision,
    ContextRecall,
)
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


# --- PATH LOGIC (Flattened) ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
RESULTS_FILE = os.path.join(CURRENT_DIR, "rag_results.json")
REPORT_FILE = os.path.join(CURRENT_DIR, "ragas_report.md")


def generate_markdown_report(df, summary_scores):
    """
    Creates a professional-grade Markdown report of the evaluation.
    """
    with open(REPORT_FILE, "w") as f:
        f.write("# üìä AuditAI: RAG Evaluation Report\n\n")
        f.write("Generated on: " + pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
        
        # --- Section 1: Executive Summary ---
        f.write("## üèõÔ∏è Executive Summary\n")
        f.write("Below are the average scores across all evaluated metrics.\n\n")
        
        f.write("| Metric | Score | Status |\n")
        f.write("| :--- | :--- | :--- |\n")
        for metric, score in summary_scores.items():
            status = "‚úÖ Passing" if score >= 0.7 else "‚ö†Ô∏è Needs Review"
            if score < 0.3: status = "‚ùå Failing"
            f.write(f"| **{metric.replace('_', ' ').title()}** | `{score:.4f}` | {status} |\n")
        
        f.write("\n---\n\n")
        
        # --- Section 2: Detailed Performance ---
        f.write("## üìù Detailed Performance Breakdown\n\n")
        
        for i, row in df.iterrows():
            f.write(f"### Question {i+1}\n")
            f.write(f"**Question:** {row['question']}\n\n")
            f.write(f"**AI Answer:** {row['answer']}\n\n")
            f.write(f"**Ground Truth:** {row['ground_truth']}\n\n")
            
            f.write("**Scores:**\n")
            f.write(f"- Faithfulness: `{row['faithfulness']:.4f}`\n")
            f.write(f"- Answer Relevancy: `{row['answer_relevancy']:.4f}`\n")
            f.write(f"- Context Precision: `{row['context_precision']:.4f}`\n")
            f.write(f"- Context Recall: `{row['context_recall']:.4f}`\n\n")
            f.write("---\n\n")

    report_path = REPORT_FILE
    print(f"‚úÖ Complete report generated: '{report_path}'")


def run_ragas_eval():
    # 1. Load Data (Generated by collector.py)
    if not os.path.exists(RESULTS_FILE):
        print(f"‚ùå Error: '{RESULTS_FILE}' not found.")
        print("   Please run 'python evals/collector.py' first to generate the dataset.")
        return

    with open(RESULTS_FILE, "r") as f:
        raw_data = json.load(f)

    print(f"üìÇ Loaded {len(raw_data)} records from {RESULTS_FILE}")

    # ‚ö†Ô∏è LIMIT DATASET TO SAVE TOKENS (Rate Limit Protection)
    # Increased to 10 for a more complete report
    raw_data = raw_data[:10]
    print(f"‚ö†Ô∏è  Running on {len(raw_data)} records...")

    # 2. Prepare Dataset for RAGAS
    ragas_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": [],
    }

    for entry in raw_data:
        ragas_data["question"].append(entry["question"])
        ragas_data["answer"].append(entry["answer"])
        ragas_data["contexts"].append(entry["contexts"])
        ragas_data["ground_truth"].append(entry["ground_truth"])

    dataset = Dataset.from_dict(ragas_data)

    # 3. Configure Judge Models (Gemini 2.5 Flash Lite)
    judge_llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=0,
        google_api_key=os.getenv("GOOGLE_API_KEY"),
    )

    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/gemini-embedding-001",
        google_api_key=os.getenv("GOOGLE_API_KEY"),
    )

    # 4. Run Evaluation
    print("üöÄ Starting RAGAS Evaluation...")
    results = evaluate(
        dataset=dataset,
        metrics=[
            Faithfulness(),
            AnswerRelevancy(),
            ContextPrecision(),
            ContextRecall(),
        ],
        llm=judge_llm,
        embeddings=embeddings,
        raise_exceptions=False,
        run_config=RunConfig(max_workers=1),
    )

    # 5. Generate Reports
    print("\n--- üìä RAGAS SCORES ---")
    summary = {
        "faithfulness": results["faithfulness"],
        "answer_relevancy": results["answer_relevancy"],
        "context_precision": results["context_precision"],
        "context_recall": results["context_recall"],
    }
    print(summary)

    df = results.to_pandas()
    df.to_csv("ragas_results.csv", index=False)
    
    # NEW: Generate the Markdown Report
    generate_markdown_report(df, summary)


if __name__ == "__main__":
    run_ragas_eval()