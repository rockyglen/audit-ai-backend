import sys
import os

# --- PATH HACK (Industrial Standard for standalone scripts) ---
# Adds the 'src' directory to the path so we can import 'audit_ai'
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
sys.path.append(os.path.join(PROJECT_ROOT, "src"))

import json
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.run_config import RunConfig
from ragas.metrics import (
    Faithfulness,
    AnswerRelevancy,
    ContextPrecision,
    ContextRecall,
)
from audit_ai.config import EVAL_JUDGE_MODEL, GOOGLE_API_KEY, EMBEDDING_MODEL
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from dotenv import load_dotenv
import numpy as np
import warnings

# Suppress deprecation warnings from RAGAS/Google
warnings.filterwarnings("ignore")

# Load environment variables
load_dotenv()


# --- PATH LOGIC (Flattened) ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
RESULTS_FILE = os.path.join(CURRENT_DIR, "rag_results.json")
REPORT_FILE = os.path.join(CURRENT_DIR, "ragas_report.md")


def generate_markdown_report(df, averages):
    """
    Creates a professional-grade Markdown report of the evaluation.
    """
    with open(REPORT_FILE, "w") as f:
        f.write("# üìä AuditAI: RAG Evaluation Report\n\n")
        f.write("Generated on: " + pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
        
        # --- Section 1: Executive Summary ---
        f.write("## üèõÔ∏è Executive Summary\n")
        f.write("Below are the average scores across all evaluated metrics.\n\n")
        
        f.write("| Metric | Score | Status |\n")
        f.write("| :--- | :--- | :--- |\n")
        
        for metric, score in averages.items():
            if np.isnan(score): 
                score = 0.0
                
            status = "‚úÖ Passing" if score >= 0.7 else "‚ö†Ô∏è Needs Review"
            if score < 0.3: status = "‚ùå Failing"
            
            metric_name = metric.replace('_', ' ').title()
            f.write(f"| **{metric_name}** | `{score:.4f}` | {status} |\n")
        
        f.write("\n---\n\n")
        
        # --- Section 2: Detailed Performance ---
        f.write("## üìù Detailed Performance Breakdown\n\n")
        
        for i, row in df.iterrows():
            f.write(f"### Question {i+1}\n")
            f.write(f"**Question:** {row['question']}\n\n")
            f.write(f"**AI Answer:** {row['answer']}\n\n")
            f.write(f"**Ground Truth:** {row['ground_truth']}\n\n")
            
            f.write("**Scores:**\n")
            for m in averages.keys():
                if m in row:
                    val = row[m]
                    val_str = f"`{val:.4f}`" if not (isinstance(val, float) and np.isnan(val)) else "`N/A`"
                    f.write(f"- {m.replace('_', ' ').title()}: {val_str}\n")
            f.write("\n---\n\n")

    print(f"‚úÖ Complete report generated: '{REPORT_FILE}'")


def run_ragas_eval():
    # 1. Load Data (Generated by collector.py)
    if not os.path.exists(RESULTS_FILE):
        print(f"‚ùå Error: '{RESULTS_FILE}' not found.")
        print("   Please run 'python evals/collector.py' first to generate the dataset.")
        return

    with open(RESULTS_FILE, "r") as f:
        raw_data = json.load(f)

    print(f"üìÇ Loaded {len(raw_data)} records from {RESULTS_FILE}")

    # ‚ö†Ô∏è LIMIT DATASET TO SAVE TOKENS (Rate Limit Protection)
    raw_data = raw_data[:10]
    print(f"‚ö†Ô∏è  Running on {len(raw_data)} records...")

    # 2. Prepare Dataset for RAGAS
    dataset = Dataset.from_dict({
        "question": [e["question"] for e in raw_data],
        "answer": [e["answer"] for e in raw_data],
        "contexts": [e["contexts"] for e in raw_data],
        "ground_truth": [e["ground_truth"] for e in raw_data],
    })

    # 3. Configure Judge Models from config
    judge_llm = ChatGoogleGenerativeAI(
        model=EVAL_JUDGE_MODEL,
        temperature=0,
        google_api_key=GOOGLE_API_KEY,
    )

    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=GOOGLE_API_KEY,
    )

    # 4. Run Evaluation
    print(f"üöÄ Starting RAGAS Evaluation using {EVAL_JUDGE_MODEL}...")
    results = evaluate(
        dataset=dataset,
        metrics=[
            Faithfulness(),
            AnswerRelevancy(),
            ContextPrecision(),
            ContextRecall(),
        ],
        llm=judge_llm,
        embeddings=embeddings,
        raise_exceptions=False,
        run_config=RunConfig(max_workers=1),
    )

    # 5. Generate Reports
    df = results.to_pandas()
    df_input = dataset.to_pandas()
    
    # CRITICAL: Re-attach input columns if RAGAS stripped them (Fixes KeyError: 'question')
    for col in ["question", "answer", "ground_truth"]:
        if col not in df.columns and col in df_input.columns:
            df[col] = df_input[col]
    
    # Calculate absolute averages from the dataframe (safest method)
    numeric_df = df.select_dtypes(include=[np.number])
    averages = numeric_df.mean().to_dict()

    print("\n--- üìä FINAL AVERAGES ---")
    for k, v in averages.items():
        print(f"{k}: {v:.4f}")

    df.to_csv("ragas_results.csv", index=False)
    
    # NEW: Generate the Markdown Report
    generate_markdown_report(df, averages)


if __name__ == "__main__":
    run_ragas_eval()